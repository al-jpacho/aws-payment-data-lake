{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PaymentsBronzeTest\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_transactions_df = spark.read.csv(\n",
    "    \"../data/raw/transactions/ingest_date=2025-09-20/transactions_2025-09-20.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "raw_transactions_df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning / transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast numeric and timestamp fields\n",
    "\n",
    "Input: amount (string/float), txn_ts (string).\n",
    "\n",
    "Output:\n",
    "\n",
    "amount → DecimalType(12,2)\n",
    "\n",
    "txn_ts → TimestampType\n",
    "\n",
    "Why: Ensures schema consistency and numeric precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def cast_dtypes(df):\n",
    "    \"\"\"\n",
    "    Cast columns to correct data types for Bronze layer.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input dataframe with raw schema.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Dataframe with amount cast to Decimal(12,2)\n",
    "        and txn_ts cast to Timestamp.\n",
    "    \"\"\"\n",
    "    cast_df = (\n",
    "        df\n",
    "        .withColumn(\"amount\", F.col(\"amount\").cast(DecimalType(12,2)))\n",
    "        .withColumn(\"txn_ts\", F.to_timestamp(\"txn_ts\"))\n",
    "    )\n",
    "    return cast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dtypes(raw_transactions_df).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_strings(df):\n",
    "    \"\"\"Normalise string columns in the DataFrame by trimming whitespace and converting to uppercase.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame with string columns to normalize.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with normalised string columns.\n",
    "    \"\"\"\n",
    "     \n",
    "    string_cols = [field.name for field in df.schema.fields if field.dataType == 'string']\n",
    "\n",
    "    for col in string_cols: \n",
    "        df = df.withColumn(col, F.upper(F.trim(F.col(col))))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_strings(raw_transactions_df).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "def deduplicate_df(df): \n",
    "    \"\"\"\n",
    "    Deduplicate the dataframe based on txn_id, keeping the most recent txn_ts.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input dataframe with possible duplicates.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Deduplicated dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    window = Window.partitionBy(\"txn_id\").orderBy(F.col(\"txn_ts\").desc())\n",
    "\n",
    "    deduped_df = (df\n",
    "                      .withColumn(\"row_num\", F.row_number().over(window))\n",
    "                      .filter(F.col(\"row_num\") == 1)\n",
    "                      .drop(\"row_num\")\n",
    "                      )\n",
    "\n",
    "    return deduped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicate_df(raw_transactions_df).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_txn_date(df):\n",
    "    \"\"\"\n",
    "    Derive txn_date column from txn_ts.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input dataframe with txn_ts column.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Dataframe with derived txn_date column.\n",
    "    \"\"\"\n",
    "    txn_date_df = df.withColumn(\"txn_date\", F.to_date(\"txn_ts\"))\n",
    "    return txn_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derive_txn_date(raw_transactions_df).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANING_FUNCTIONS = {\n",
    "    cast_dtypes,\n",
    "    normalise_strings,\n",
    "    deduplicate_df,\n",
    "    derive_txn_date\n",
    "}\n",
    "\n",
    "def apply_cleaning_functions(df, functions):\n",
    "    \"\"\"\n",
    "    Apply a set of cleaning functions to a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input dataframe to be cleaned.\n",
    "        functions (set): Set of cleaning functions to apply.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Cleaned dataframe.\n",
    "    \"\"\"\n",
    "    for func in functions:\n",
    "        df = func(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_transactions_df = apply_cleaning_functions(raw_transactions_df, CLEANING_FUNCTIONS)\n",
    "\n",
    "cleaned_transactions_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
